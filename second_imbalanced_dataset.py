# -*- coding: utf-8 -*-
"""second imbalanced dataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15pb7vLeZ4nuB_hBCo6QG9Yw3z9LUM4bF
"""

import pandas as pd

# Load dataset
df = pd.read_csv("/content/drive/MyDrive/ dataset imbalanced/imbalanced_dataset.csv")

# Explore data
print(df.info())
print(df['Class'].value_counts())
print(df.head())

"""# ***Remove Duplicate Rows***"""

# Remove duplicate rows
df = df.drop_duplicates()
print(f"After removing duplicates: {df.shape}")

"""# ***Handle Missing Values***"""

# Check for missing values
print(df.isnull().sum())

# Option 1: Drop rows with missing values
df = df.dropna()

# Option 2 (alternate): Fill missing values with mean (if any)
# df.fillna(df.mean(), inplace=True)

"""# ***Preprocessing and Splitting***"""

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Split features and target
X = df.drop('Class', axis=1)
y = df['Class']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

# Feature scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

"""# ***Apply 7 Balancing Techniques and Train Model***"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix
from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN
from imblearn.under_sampling import RandomUnderSampler, TomekLinks, EditedNearestNeighbours
from imblearn.combine import SMOTETomek

# Classifier
clf = LogisticRegression(solver='liblinear', random_state=42)

# Store results
results = {}
conf_matrices = {}

# Define balancing methods
balancing_techniques = {
    "Original": (X_train_scaled, y_train),
    "RandomOverSampler": RandomOverSampler(random_state=42).fit_resample(X_train_scaled, y_train),
    "SMOTE": SMOTE(random_state=42).fit_resample(X_train_scaled, y_train),
    "ADASYN": ADASYN(random_state=42).fit_resample(X_train_scaled, y_train),
    "RandomUnderSampler": RandomUnderSampler(random_state=42).fit_resample(X_train_scaled, y_train),
    "SMOTE + TomekLinks": SMOTETomek(random_state=42).fit_resample(X_train_scaled, y_train),
    "EditedNearestNeighbours": EditedNearestNeighbours().fit_resample(X_train_scaled, y_train)
}

# Train and evaluate
for name, (X_res, y_res) in balancing_techniques.items():
    clf.fit(X_res, y_res)
    y_pred = clf.predict(X_test_scaled)
    acc = accuracy_score(y_test, y_pred)
    cm = confusion_matrix(y_test, y_pred)
    results[name] = acc
    conf_matrices[name] = cm

"""# ***Plot Accuracy Comparison***"""

import matplotlib.pyplot as plt

# Bar plot of accuracy scores
plt.figure(figsize=(10, 5))
plt.bar(results.keys(), results.values(), color='skyblue')
plt.ylabel("Accuracy")
plt.title("Accuracy of Logistic Regression with Different Balancing Techniques")
plt.xticks(rotation=45)
plt.grid(True)
plt.tight_layout()
plt.show()

"""# ***Plot Confusion Matrices***"""

num_methods = len(conf_matrices)
cols = 3  # Number of columns in the grid
rows = (num_methods + cols - 1) // cols  # Auto calculate number of rows

# Create subplots
fig, axes = plt.subplots(rows, cols, figsize=(15, 4 * rows))
axes = axes.flatten()  # Flatten in case of single row

# Plot each confusion matrix
for i, (name, cm) in enumerate(conf_matrices.items()):
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[i])
    axes[i].set_title(f"Confusion Matrix: {name}")
    axes[i].set_xlabel("Predicted")
    axes[i].set_ylabel("Actual")

# Hide any empty subplots
for j in range(i + 1, len(axes)):
    fig.delaxes(axes[j])

plt.tight_layout()
plt.show()

""" ✅ Conclusion
In this project, we addressed the challenge of working with a highly imbalanced dataset, where the minority class was significantly underrepresented. Several resampling techniques were applied, including Random Over Sampling, SMOTE, ADASYN, Random Under Sampling, SMOTE + Tomek Links, and Edited Nearest Neighbors (ENN) to balance the dataset and improve classification performance.

The results clearly demonstrated that:

The original model (without resampling) performed poorly in detecting the minority class, showing high accuracy but very low recall.

SMOTE and ADASYN significantly improved the model’s ability to identify the minority class by synthetically generating new examples, with SMOTE showing slightly more stable results.

Under sampling methods balanced the dataset by reducing the majority class, but at the cost of potentially important information loss.

SMOTE + Tomek Links and ENN provided a good balance between data cleaning and synthetic generation, helping improve precision and recall while reducing noise.
"""